---
title: "Poner como título los ejercicios que vayas a realizar"
author: "Tu nombre"
date: "La fecha"
---

1. Carga los datos en el entorno de Rstudio a través de la función readRDS (nota: como estás trabajando con datos “conocidos” no es necesario revisar el tipo de las variables, pues lo hiciste el primer día, pero se debe hacer siempre que el conjunto de datos es nuevo). Formatea la variable dependiente si fuera necesario.
```{r }
library(caret)
library(pROC)
library(car)
library(purrr)
library(furrr)
library(dplyr)
library(glmnet)
# Para paralelizar
library(doParallel)
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)
datos<-readRDS("DatosGSS")
str(datos)
```

```{r}
datos$Hijos<-as.factor(make.names(datos$Hijos))
table(datos$Hijos)/nrow(datos)
```
2. Realiza una partición del conjunto de datos en entrenamiento (80%) y prueba (20%).
```{r}
set.seed(12345)
trainIndex <- createDataPartition(datos$Hijos, p=0.8, list=FALSE)
data_train <- datos[trainIndex,]
data_test <- datos[-trainIndex,]
```

3. Utilizando los datos de la partición de entrenamiento, construye 6 modelos de regresión logística binaria para la variable Hijos aplicando los 3 métodos de selección de variables estudiados y los 2 criterios de selección a partir de la función step. ¿Cuántos parámetros tienen los modelos? ¿Cuántos modelos diferentes se generan? ¿A qué se deben las diferencias (sobre todo entre los modelos generados con AIC y los generados con BIC)?

```{r}
null<-glm(Hijos~1,data=data_train, family=binomial)
full<-glm(Hijos~.,data=data_train, family=binomial)
modeloStepBIC<-step(null, scope=list(lower=null, upper=full), direction="both",
                    k=log(nrow(data_train)), trace=F)
modeloStepAIC<-step(null, scope=list(lower=null, upper=full), direction="both",
                    k=2, trace=F)
modeloForwBIC<-step(null, scope=list(lower=null, upper=full), direction="forward",
                    k=log(nrow(data_train)),trace=F)
modeloForwAIC<-step(null, scope=list(lower=null, upper=full), direction="forward",
                    k=2, trace=F)
modeloBackBIC<-step(full, scope=list(lower=null, upper=full), direction="backward",
                    k=log(nrow(data_train)),trace=F)
modeloBackAIC<-step(full, scope=list(lower=null, upper=full), direction="backward",
                    k=2, trace=F)

```

```{r}
modelos<-list(modeloStepBIC,modeloStepAIC,modeloForwBIC,modeloForwAIC,modeloBackBIC,
              modeloBackAIC)
map(modelos,function(x) formula(x)) 
```
```{r}
map_int(modelos,function(x) x$rank)
```

4. Una vez generados los modelos (los de la selección automática y el que creaste manualmente con las variables EstadoCivil, Edad y Raza), se debe determinar cuál es el mejor de todos ellos, para lo cual debes aplicar validación cruzada repetida (utiliza un bucle para simplificar esta tarea).
Genera los boxplots para las 3 medidas (AUC, tasa de acierto e índice Kappa), así como los resúmenes y compara los modelos. ¿Cuál parece ser el mejor? Recuerda que si varios modelos son parecidos en cuanto a capacidad predictiva se debe escoger el más sencillo (el que tenga menos parámetros).

```{r}
Manual <- lm(Hijos ~ EstadoCivil + Edad + Raza, data = data_train)
Manual$rank
```
```{r}
modelos<-list(Manual, modeloStepBIC, modeloStepAIC)
vcrTodosModelos<-list()
for (i in 1:length(modelos)){
  set.seed(12345)
  vcr<-train(formula(modelos[[i]]), data = data_train,
           method = "glm", family="binomial",
           trControl = trainControl(method="repeatedcv", number=5, repeats=20,
                                    summaryFunction=multiClassSummary, 
                                    classProbs=TRUE, savePredictions = TRUE)
)
  vcrTodosModelos[[i]]<-vcr
}
names(vcrTodosModelos)<-paste0("Model",1:length(modelos), "_",sapply(modelos,function(x) x$rank))
bwplot(resamples(vcrTodosModelos), metric=c("AUC", "Kappa", "Accuracy"),
       scales = list(x = list(relation = "free")))

```

5. Obtén la matriz de confusión utilizando los datos de la partición de prueba del modelo “ganador”, así como un resumen de sus estadísticos y el AUC. ¿Qué puedes decir de la calidad del modelo?
```{r}
summary(resamples(vcrTodosModelos), metric=c("AUC", "Kappa", "Accuracy"))
```

```{r}
probs_test <-predict(modeloStepBIC,data_test, type="response")
cm_test<-confusionMatrix(data=as.factor(ifelse(probs_test>=0.73,"X1","X0")),
                      reference=data_test$Hijos, positive="X1")
cm_test$table
cm_test$overall[1:2]
cm_test$byClass[1:2]
curvaROC_test<-roc(data_test$Hijos, probs_test)
curvaROC_test$auc
```

6. Para finalizar, realiza un análisis de tipo II sobre el modelo, saca las conclusiones oportunas e interpreta los odds-ratio.

```{r}
summary(modeloStepBIC)
```
```{r}
exp(coef(modeloStepBIC))
```
7. Construye un modelo LASSO y otro Ridge con todos los valores posibles de λ y representa gráficamente los resultados. Comenta lo que observes y, basándote en dicha información, explica los fundamentos de ambos modelos incidiendo es sus diferencias.

```{r}
y <- data_train$Hijos
x<-model.matrix(Hijos~., data=data_train)[,-1]

modeloLASSO<-glmnet(x, y, alpha = 1, family = "binomial")
plot(modeloLASSO, xvar="lambda", ylim=c(-3,3), main = "LASSO")

``` 

```{r}
modeloRIDGE<-glmnet(x, y, alpha = 0, family = "binomial")
plot(modeloRIDGE, xvar="lambda", ylim=c(-3,3), main = "Ridge")

```







8. Para determinar el modelo penalizado óptimo, aplica validación cruzada para una combinación amplia de α y λ y representa gráficamente los resultados. Indica qué significa la línea horizontal que se añade en el gráfico y comenta los resultados que observas.
Determina la combinación de parámetros óptima según la regla 1se y, posteriormente, construye el modelo definitivo. Obtén los parámetros del modelo y comenta su efecto sobre la variable objetivo.











9. Obtén la matriz de confusión utilizando los datos de la partición de prueba del modelo anterior, así como un resumen de sus estadísticos. Calcula también su AUC. ¿Qué puedes decir de la calidad del modelo?















